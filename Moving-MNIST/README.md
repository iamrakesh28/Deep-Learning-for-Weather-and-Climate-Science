# Moving MNIST

Moving MNIST Dataset consists of sequences of 20 image frames showing 2 handwritten digits
from MNIST database moving in the 64 x 64 frame. Each sequence is generated by randomly
choosing 2 digits from the MNIST database and each of them is assigned a random velocity
in the beginning. The digits move inside the frame and bounce back when they reaches the
edge of the frame. Moving MNIST dataset of 10,000 sequences can also be found at 
http://www.cs.toronto.edu/~nitish/unsupervised_video/

The Moving MNIST dataset frames has 256 levels (0 - 255). The frames were thresholded
to have binary intensities (0 and 1). Each 20 frames sequence was splited into two sequence
of 10 frames (first 10 frames for input and other 10 for prediction). The models were trained
to minimize the binary crossentropy loss and RMSProp optimizer (learning rate = 0.001 and
decay rate = 0.9) was used during the training. Also, we performed early stopping on the
validation set.

* 2 Layers Encoder-Decoder network with 32, 32 hidden units and (5 x 5) filter size in each
layer: This model was trained over 1,000 sequences and tested on 200 sequences (200
validation sequences). The average binary crossentropy loss was 0.2998.

![gif1](https://github.com/iamrakesh28/Deep-Learning-for-Weather-and-Climate-Science/blob/master/Moving-MNIST/images/32_32/7_9.gif) 
![gif1](https://github.com/iamrakesh28/Deep-Learning-for-Weather-and-Climate-Science/blob/master/Moving-MNIST/images/32_32/1_6.gif) </br> 

## Image Reshaping
The above models were trained on NVidia K80 GPU provided by Kaggle with a weekly quota of
around 40 hours. The weights in the ConvLSTM cells depends on batch_size x image_rows
x image_cols x filters. Training deeper models with many hidden units required good
memory space. So, deeper Encoder-Decoder model couldnâ€™t be trained due to the limited re-
source. Since, the memory allocated for ConvLSTM depends on image_size (rows x cols),
the 64 x 64 frames were reshaped into 16 x 16 x 16 frame (increasing the channel dimension).
This allowed to train deeper Encoder-Decoder networks. Two Encoder-Decoder networks were
trained on the sequences (8,000 training sequences, 1,000 validation sequences and 1,000 test
sequences).
* 2 Layers with 128, 128 hidden units and (5 x 5) filter size in each layer. The average
binary crossentropy loss was 0.2791.

![gif2](https://github.com/iamrakesh28/Deep-Learning-for-Weather-and-Climate-Science/blob/master/Moving-MNIST/images/128_128/7_9.gif)
![gif2](https://github.com/iamrakesh28/Deep-Learning-for-Weather-and-Climate-Science/blob/master/Moving-MNIST/images/128_128/1_6.gif) </br>

* 3 Layers with 128, 64, 64 hidden units and (5 x 5) filter size in each layer. The average
binary crossentropy loss was 0.2662.

![gif3](https://github.com/iamrakesh28/Deep-Learning-for-Weather-and-Climate-Science/blob/master/Moving-MNIST/images/128_64_64/7_9.gif)
![gif3](https://github.com/iamrakesh28/Deep-Learning-for-Weather-and-Climate-Science/blob/master/Moving-MNIST/images/128_64_64/1_6.gif)

